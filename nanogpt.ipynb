{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "\n",
    "Authors: Yunhao Cao, Carl(Qianxin Gan), Kevin(Yuxi) Liu\n",
    "\n",
    "In this assignment you will learn to implement a GPT model from scratch. This includes implementing the Transformer architecture (with causal input mask), the GPT model, a byte-level BPE tokenizer, the embedding layer, and the positional encoding layer.\n",
    "We will train a GPT-2 Model that you can play around on your own! Here's the original paper for reference: ([Original Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License Information\n",
    "Copyright 2023 Yunhao Cao, Qianxin Gan, Yuxi Liu\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from grader import *\n",
    "import typing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup autograder submission\n",
    "submitter = AutograderSubmitter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenizer\n",
    "\n",
    "To transform text into a format that can be used by a neural net, we need to first tokenize it (That is, transform the text corpus into indexes of our dictionary). The GPT-2 model uses a byte-level BPE tokenizer. \n",
    "\n",
    "Before we start, please first take a look at [this tutorial video by Huggingface](https://youtu.be/HEikzVL-lZU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial video above, you see that a BPE tokenizer starts with a base dictionary set of characters. Those \"base set of characters\" are usually represented by unicode characters. Unicode characters are the standard way to represent all possible human languages plus our favorite emojis ðŸ˜€ðŸ˜™ in byte streams. There are multiple Unicode standards: UTF-8, UTF-16, UTF-32, etc. However, because unicode characters are not very memory efficient, this basically means that we need to start with a **huge** base dictionary size to begin with our tokenizer. And this is why the authors of GPT-2 chose to instead use a byte-level BPE tokenizer. That is, we split characters into futher smaller fragments (1 byte, or 8-bits) and use those as our base dictionary. This way, we can reduce the base dictionary size from 100,000+ unicode characters to just 256.\n",
    "\n",
    "> As a side note, the UTF-8 standard is not a strict 8-bit-per-character standard and each character in a UTF-8 stream can take more than 8 bits. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a byte-level BPE tokenizer, we need to first create a base dictionary of 256 characters. This base dictionary will be constructed and passed into our `Dictionary` class via the `__init__` method. Then we need to be able to expand our vocabulary list, one at a time. We will do this by implementing both the `expand_dictionary` method and the `find_combinations_to_expand` method.\n",
    "\n",
    "Basically, think about our current dictionary as all possible alphabets and a space. We would first tokenize our text into a list of indexes in our dictionary. Then we will enumerate through the list of indexes and find the most frequent pair of indexes that appear next to each other. We will then combine those two indexes (of the most frequent pair) into a new index and add it to our dictionary. We will repeat this process until we reach our desired vocabulary size.\n",
    "\n",
    "There are two member attributes of the `Dictionary` class, `dictionary_array` and `combinations_to_index`. The `dictionary_array` attribute simply holds all the vocabularies and `combinations_to_index` is used later in `tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, base_dictionary : typing.List[bytes] = [i.to_bytes(1,'big') for i in range(256)]) -> None:\n",
    "        \n",
    "        # dictionary holds all volcabulary items and the index of each item in this array will be the input idx to the model\n",
    "        self.dictionary_array : typing.List[bytes] = base_dictionary.copy()\n",
    "\n",
    "        # This is a dictionary that maps a combination of two vocab items to a later vocab item\n",
    "        self.combinations_to_index : typing.Dict[typing.Tuple[int, int], int] = {}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dictionary_array)\n",
    "    \n",
    "    def __getitem__(self, key: int) -> str:\n",
    "        return self.dictionary_array[key]\n",
    "    \n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.dictionary_array\n",
    "    \n",
    "    def expand_dictionary(self, combination_vocab : typing.Tuple[int, int]) -> None:\n",
    "        \"\"\"\n",
    "        This function should expand the dictionary with one more vocabulary item, \n",
    "        the item should be the concatenation of the two vocab items in combination_vocab\n",
    "        You need to modify both the dictionary_array and combinations_to_index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combination_vocab : typing.Tuple[int, int]\n",
    "            The combination of two vocab items to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # self.dictionary_array.append(self.dictionary_array[combination_vocab[0]] + self.dictionary_array[combination_vocab[1]])\n",
    "        # self.combinations_to_index[combination_vocab] = len(self.dictionary_array) - 1\n",
    "    \n",
    "\n",
    "    def find_combination_to_expand(self, corpus_of_text: typing.List[int]) -> typing.Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        This function should find the combination of two vocab items that occurs the most in the corpus of text and return it\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus_of_text : typing.List[int]\n",
    "            The corpus of text represented by a list of integers (with each integer representing a vocab in the dictionary) to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # count_dict = {}\n",
    "        # for i in range(len(corpus_of_text) - 1):\n",
    "        #     if (corpus_of_text[i], corpus_of_text[i+1]) in count_dict:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] += 1\n",
    "        #     else:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] = 1\n",
    "        # return max(count_dict, key=count_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "grade_dictionary_class_expand_dictionary(Dictionary())\n",
    "grade_dictionary_class_find_combination_to_expand(Dictionary())\n",
    "submitter.submission_data[\"tokenizer_comb\"] = generate_dictionary_class_find_combination_to_expand_dat(\n",
    "    Dictionary()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we're able to expand our vocabulary list. But how do we tokenize our text into a list of indexes? We will do this by implementing the `tokenize` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text : str, dictionary : Dictionary) -> typing.List[int]:\n",
    "    \"\"\"\n",
    "    This function should tokenize the text using the dictionary and return the tokenized text as a list of integers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to tokenize\n",
    "    \n",
    "    dictionary : Dictionary\n",
    "        The dictionary to use for tokenization\n",
    "    \"\"\"\n",
    "\n",
    "    text_bytestream = bytes(text, \"utf-8\") # convert text to bytestream\n",
    "    tokenized_text : typing.List[int] = [] # initialize tokenized text\n",
    "    for i in range(len(text_bytestream)):\n",
    "        tokenized_text.append(\n",
    "            dictionary.dictionary_array.index(text_bytestream[i:i+1])\n",
    "        )\n",
    "    \n",
    "    num_tokenized_last_pass = len(tokenized_text)\n",
    "    # We will sweep through the tokenized text and replace any combination of two vocab items with the later vocab item\n",
    "    while num_tokenized_last_pass > 0:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # num_tokenized_last_pass = 0\n",
    "        # new_tokenized_text = []\n",
    "        # for i in range(len(tokenized_text) - 1):\n",
    "        #     if (tokenized_text[i], tokenized_text[i+1]) in dictionary.combinations_to_index:\n",
    "        #         new_tokenized_text.append(dictionary.combinations_to_index[(tokenized_text[i], tokenized_text[i+1])])\n",
    "        #         num_tokenized_last_pass += 1\n",
    "        #     else:\n",
    "        #         new_tokenized_text.append(tokenized_text[i])\n",
    "        #         if i == len(tokenized_text) - 2:\n",
    "        #             new_tokenized_text.append(tokenized_text[i+1])\n",
    "        \n",
    "        # tokenized_text = new_tokenized_text\n",
    "    \n",
    "    return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_tokenizer(tokenize, Dictionary())\n",
    "submitter.submission_data[\"tokenizer\"] = generate_tokenizer_submission(tokenize, Dictionary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Embeddings\n",
    "Next we will implement the token embeddings and positional embeddings. The positional embeddings are used to encode the position of each token in the sequence. The positional embeddings are added to the token embeddings before being passed into the Transformer.\n",
    "\n",
    "We will represent our token embeddings as a `torch.nn.Embedding` layer. The positional embeddings will be represented as fixed buffer just like in the original `Attention is all you need` paper.\n",
    "\n",
    "Please notice that formula for position embedding is\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension.\n",
    "\n",
    "You just need to implement initilization of the PE matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout = 0.1, max_len: int = 1024):\n",
    "        super().__init__()\n",
    "        pe = self.calculate_pe(d_model, max_len)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def calculate_pe(self, d_model : int, max_len : int):\n",
    "        \"\"\"\n",
    "        Calculate positional encoding for transformer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            The dimension of each embedding token\n",
    "        \n",
    "        max_len : int\n",
    "            The maximum length of the sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pe : Tensor\n",
    "            The positional encoding tensor of shape ``[1, max_len, d_model]``\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # position = torch.arange(max_len).unsqueeze(1)\n",
    "        # div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).unsqueeze(0)\n",
    "        # pe = torch.zeros(1, max_len, d_model)\n",
    "        # combined_term = position * div_term\n",
    "        # pe[0, :, 0::2] = torch.sin(combined_term)\n",
    "        # pe[0, :, 1::2] = torch.cos(combined_term)\n",
    "        # return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.shape[1],:]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, max_len: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, dropout, max_len)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokens: Tensor, shape ``[batch_size, seq_len]``\n",
    "        \"\"\"\n",
    "        return self.pe(self.embedding(tokens) * math.sqrt(self.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "grade_pe(PositionalEncoding(6,0.0,128))\n",
    "submitter.submission_data[\"PE\"] = generate_pe_sub(PositionalEncoding(12,0.0,256))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Transformer Layer\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Great job on implementing Part 1! In Part 2, we will implement the transformer layer. The first step in building the transformer layer is multi-head attention. Let's understand how attention works. The motivation behind attention is that some parts of our input data are more important than others. We only want to focus on the important parts of the data. The attention mechanism achieves this by using a set of queries, keys, and values generated from the input tokens. A similarity score is produced between a query $q_i$ and a key $k_j$ and higher similiarity scores means token $i$ should attend to token $j$ more. \n",
    "\n",
    "GPT-2 is an auto-regressive model (i.e. it predicts future tokens given previous data). GPT-2 uses masked attention because we want to mask the information about future tokens. If we had information about future tokens, this would be considered cheating because the model would know what to output. GPT-2 uses multi-headed attention, or multiple attention heads. Using multiple heads is useful because it enables the model to attend to parts of the sequence differently. You can think of it as enabling more expressivity in communication amongst tokens.\n",
    "\n",
    "In this section, we implement multi-headed attention head in the following steps:\n",
    "1. Generate queries, keys, and values from the input tokens (computed in the previous part)\n",
    "2. Use the queries, keys, and values to compute masked attention\n",
    "3. Apply a linear projection and dropout to the masked attention\n",
    "\n",
    "Here is how attention is calculated: \n",
    "\n",
    "$$\n",
    "Attention = Softmax(Mask(\\frac{QK^T}{\\sqrt{d_{head}}}))V\n",
    "$$\n",
    "where $d_{head}$ is the embedding dimension of each head, $n_{heads}$ is the number of heads, and $d_{model} = n_{heads} * d_{head}$.\n",
    "\n",
    "**Note:** The input `x` is of size `(batch_size, seq_len, d_model)` and the output is of size `(batch_size, tgt_len, d_model)`. However, the attention function should take in queries of size `(batch_size, n_heads, tgt_len, d_head)` and keys and values of size `(batch_size, n_heads, seq_len, d_head)`. Thus, we need to rearrange the input and attention output somehow to match the different dimensions. Also note that for self-attention, `tgt_len == seq_len` since we are passing the input `x` as the queries, keys, and values.\n",
    "\n",
    "*Hint 1: `einops.rearrange` may be helpful here. You can reference the documentation here: https://einops.rocks/api/rearrange/. We've included the `einops` package for you already.*\n",
    "\n",
    "*Hint 2: To apply the mask before applying softmax, we want to set the weights we want to mask out to $-inf$ so that softmax ignores those weights. We want something like $\\begin{bmatrix} W_{11} & -inf & -inf & ...\\\\ W_{21} & W_{22} & -inf & ...\\\\ W_{31} & W_{32} & W_{33} & ...\\\\ ... & ... & ... & ... \\\\ \\end{bmatrix}$ where $W = \\frac{QK^T}{\\sqrt{d_{head}}}$. `torch.tril` and `torch.Tensor.masked_fill` may be helpful here.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.output_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def compute_masked_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate masked attention for transformer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Q: Input tensor of queries with size (batch_size, n_heads, tgt_len, d_head)\n",
    "        K: Input tensor of keys with size (batch_size, n_heads, seq_len, d_head)\n",
    "        V: Input tensor of values with size (batch_size, n_heads, seq_len, d_head)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output attention tensor with size (batch_size, n_heads, tgt_len, d_head)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # _, _, seq_len, d_head = Q.shape\n",
    "        # weights = Q @ K.transpose(-2, -1) / (d_head ** 0.5)\n",
    "        # mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        # masked_weights = weights.masked_fill(mask == 0, float('-inf'))\n",
    "        # return nn.functional.softmax(masked_weights, -1) @ V\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-headed attention\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output tensor after forward pass with size (batch_size, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # Q = self.query_proj(x)\n",
    "        # K = self.key_proj(x)\n",
    "        # V = self.value_proj(x)\n",
    "        # Q = einops.rearrange(Q, 'a b (c d) -> a c b d', c=self.n_heads)\n",
    "        # K = einops.rearrange(K, 'a b (c d) -> a c b d', c=self.n_heads)\n",
    "        # V = einops.rearrange(V, 'a b (c d) -> a c b d', c=self.n_heads)\n",
    "        # attention = self.compute_masked_attention(Q, K, V)\n",
    "        # attention = einops.rearrange(attention, 'a c b d -> a b (c d)', c=self.n_heads)\n",
    "        # out = self.output_proj(attention)\n",
    "        # return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "grade_attention(MultiHeadAttention)\n",
    "submitter.submission_data[\"Attention\"] = generate_attention_sub(MultiHeadAttention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Block\n",
    "\n",
    "Awesome! Now that we have implemented multi-head attention, the next step is to implement the feed-forward block. The feed-forward block is just two linear layers with ReLU in between and dropout at the end. Based on the *Attention is all you Need (Vaswani et al.)* paper, they made the hidden layer dimension of size $4 * d_{model}$ and input and output dimensions of size $d_{model}$. \n",
    "\n",
    "**Note:** GPT-2 uses GeLU activation instead of ReLU activation. GeLU activation is slightly different in that it is smoother than ReLU. You can reference this paper (https://arxiv.org/pdf/1606.08415v3.pdf) to get a better understanding of GeLU, but we have provided the code to calculate GeLU for you already.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, 4 * d_model)\n",
    "        self.gelu = gelu\n",
    "        self.fc2 = nn.Linear(4 * d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for feed-forward block\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output tensor after forward pass with size (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.gelu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.dropout(x)\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "grade_feed_forward(FeedForward)\n",
    "submitter.submission_data[\"FeedForward\"] = generate_feed_forward_sub(FeedForward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layer\n",
    "\n",
    "Great! Now we can combine everything into one transformer block. The transformer block involves multi-head attention and the feed-forward step with layer normalizations and residual connections in between. The layer norms and residual connections help prevent vanishing and exploding gradients. We don't use batch norm here because it doesn't make sense to normalize across different examples in a batch for a single feature, especially when the examples can vary in length. \n",
    "\n",
    "Now implement the transformer layer, which is the attention block and feed-forward block with two layer-norms and residual connection in between.\n",
    "\n",
    "**Note:** GPT-2 is a bit different than the *Attention is all you Need (Vaswani et al.)* paper and adds a residual connection after the layer normalization. That is, we compute $residual + LayerNorm(x)$ instead of $LayerNorm(x + residual)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention_layer = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = FeedForward(d_model, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for transformer layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Input tensor of tokens with size (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Output tensor after forward pass with size (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # attention = self.attention_layer(x)\n",
    "        # x = x + self.norm1(attention)\n",
    "        # out = self.feed_forward(x)\n",
    "        # out = x + self.norm2(out)\n",
    "        # return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "grade_transformer_layer(TransformerLayer)\n",
    "submitter.submission_data[\"TransformerLayer\"] = generate_transformer_layer_sub(TransformerLayer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GPT-2 Model and Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome work! Now that we have all the essential components, we can proceed to implement the GPT model. The model follows the outlined structure here, but please feel free to tweak with the model afterwards.\n",
    "\n",
    "1. Token Embedding\n",
    "2. Transformer Layers\n",
    "3. Final Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 model implementation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used in the model.\n",
    "    d_model : int\n",
    "        The dimension of the token embeddings and transformer layers.\n",
    "    n_layers : int\n",
    "        The number of transformer layers in the model.\n",
    "    n_heads : int, optional, default=4\n",
    "        The number of attention heads in the multi-head self-attention mechanism.\n",
    "    dropout : float, optional, default=0.1\n",
    "        The dropout probability for the model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    token_embedding : TokenEmbedding\n",
    "        The token embedding layer.\n",
    "    transformer_layers : nn.ModuleList\n",
    "        The list of transformer layers.\n",
    "    fc : nn.Linear\n",
    "        The final linear layer that projects the output to the size of the vocabulary.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.token_embedding = ...\n",
    "        self.transformer_layers = ...\n",
    "        self.fc = ...\n",
    "        # Include other attributes if necessary\n",
    "        \n",
    "        # SOLUTION:\n",
    "        # self.token_enbedding = TokenEmbedding(vocab_size, d_model)\n",
    "        # self.transformer_layers = nn.ModuleList([TransformerLayer(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
    "        # self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the GPT model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens: torch.Tensor\n",
    "            input tensor of tokens with size (batch_size, seq_len)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after forward pass with size (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # SOLUTION:\n",
    "        # x = self.token_embedding(tokens)\n",
    "        # for layer in self.transformer_layers:\n",
    "        #     x = layer(x)\n",
    "        # x = self.fc(x)\n",
    "        # return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(model, optimizer, criterion, dataloader, epochs: int, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the GPT-2 model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : GPT2\n",
    "        The GPT-2 model to be trained.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer used for updating the model's weights.\n",
    "    criterion : torch.nn.Module\n",
    "        The loss function used for training.\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        The DataLoader used for iterating over the training data.\n",
    "    epochs : int\n",
    "        The number of epochs to train the model.\n",
    "    device : torch.device\n",
    "        The device on which the model will be trained (e.g., 'cpu' or 'cuda'),\n",
    "        defaulted to cpu.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # SOLUTION:\n",
    "    # model.train()\n",
    "    # for epoch in range(epochs):\n",
    "    #     for batch in dataloader:\n",
    "    #         tokens = batch[\"tokens\"].to(device)\n",
    "    #         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    #         optimizer.zero_grad()\n",
    "    #         logits = model(tokens)\n",
    "    #         loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Finetuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather your submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will generate a `submission.npz` file. Please submit this file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter.generate_submission_file(\"submission.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal use\n",
    "from grader_internal import Autograder\n",
    "autograder = Autograder()\n",
    "grade = autograder.grade(submitter.submission_data)\n",
    "grade_2 = autograder.grade(np.load(\"submission.npz\"))\n",
    "assert np.all(grade == grade_2)\n",
    "grade_percent = np.mean(grade) * 100\n",
    "print(\"Grade: %.2f%%\" % (grade_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
